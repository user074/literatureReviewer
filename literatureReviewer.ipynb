{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/user074/literatureReviewer/blob/main/literatureReviewer.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ],
      "metadata": {
        "id": "4hTr0jJiBlr6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This requires a GPU to run since Im using nougat-ocr, which is really good one for academic papers!"
      ],
      "metadata": {
        "id": "L5GgeodjLj4U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afZJDOahyqfU",
        "outputId": "607110e4-0f12-4933-9acc-95840cd67a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nougat-ocr\n",
            "  Downloading nougat_ocr-0.1.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.8/81.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Collecting transformers>=4.25.1 (from nougat-ocr)\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.5.4 (from nougat-ocr)\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.5/431.5 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from nougat-ocr)\n",
            "  Downloading orjson-3.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from nougat-ocr) (4.8.0.76)\n",
            "Collecting datasets[vision] (from nougat-ocr)\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning>=2.0.0 (from nougat-ocr)\n",
            "  Downloading lightning-2.0.9-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from nougat-ocr) (3.8.1)\n",
            "Collecting python-Levenshtein (from nougat-ocr)\n",
            "  Downloading python_Levenshtein-0.21.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting sentencepiece (from nougat-ocr)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sconf>=0.2.3 (from nougat-ocr)\n",
            "  Downloading sconf-0.2.5-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: albumentations>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nougat-ocr) (1.3.1)\n",
            "Collecting pypdf>=3.1.0 (from nougat-ocr)\n",
            "  Downloading pypdf-3.16.1-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.3/276.3 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdfium2 (from nougat-ocr)\n",
            "  Downloading pypdfium2-4.20.0-py3-none-manylinux_2_17_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4->nougat-ocr) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.5.4->nougat-ocr) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr) (1.11.2)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr) (0.19.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr) (6.0.1)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations>=1.0.0->nougat-ocr) (0.0.4)\n",
            "Requirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (3.1.2)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<4.0,>=2.2.1 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (8.1.7)\n",
            "Collecting croniter<1.5.0,>=1.3.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dateutils<2.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading deepdiff-6.5.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<2.0,>=0.92.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading fastapi-0.103.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (2023.6.0)\n",
            "Collecting inquirer<5.0,>=2.10.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.38 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading lightning_cloud-0.5.38-py3-none-any.whl (659 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m660.0/660.0 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.7.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (23.1)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (5.9.5)\n",
            "Requirement already satisfied: pydantic<2.2.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (1.10.12)\n",
            "Collecting python-multipart<2.0,>=0.0.5 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (13.5.2)\n",
            "Collecting starlette (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading starlette-0.31.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading torchmetrics-1.1.2-py3-none-any.whl (764 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.8/764.8 kB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (5.7.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (4.5.0)\n",
            "Requirement already satisfied: urllib3<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (2.0.4)\n",
            "Collecting uvicorn<2.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.0->nougat-ocr) (1.6.2)\n",
            "Collecting websockets<13.0 (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading pytorch_lightning-2.0.9-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Collecting ruamel.yaml (from sconf>=0.2.3->nougat-ocr)\n",
            "  Downloading ruamel.yaml-0.17.32-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting munch (from sconf>=0.2.3->nougat-ocr)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->nougat-ocr) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers>=4.25.1->nougat-ocr)\n",
            "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.1->nougat-ocr)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.25.1->nougat-ocr)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->nougat-ocr) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets[vision]->nougat-ocr)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->nougat-ocr) (1.5.3)\n",
            "Collecting xxhash (from datasets[vision]->nougat-ocr)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets[vision]->nougat-ocr)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=6.2.1 in /usr/local/lib/python3.10/dist-packages (from datasets[vision]->nougat-ocr) (9.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->nougat-ocr) (1.3.2)\n",
            "Collecting Levenshtein==0.21.1 (from python-Levenshtein->nougat-ocr)\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein==0.21.1->python-Levenshtein->nougat-ocr)\n",
            "  Downloading rapidfuzz-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<3.0,>=1.2.0->lightning>=2.0.0->nougat-ocr) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning>=2.0.0->nougat-ocr) (2.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning>=2.0.0->nougat-ocr) (2023.3.post1)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi<2.0,>=0.92.0->lightning>=2.0.0->nougat-ocr) (3.7.1)\n",
            "Collecting starlette (from lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<5.0->lightning>=2.0.0->nougat-ocr) (2.1.3)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from lightning-cloud>=0.5.38->lightning>=2.0.0->nougat-ocr) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.38->lightning>=2.0.0->nougat-ocr) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations>=1.0.0->nougat-ocr) (1.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning>=2.0.0->nougat-ocr) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning>=2.0.0->nougat-ocr) (2.16.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations>=1.0.0->nougat-ocr) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations>=1.0.0->nougat-ocr) (2.31.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations>=1.0.0->nougat-ocr) (2023.8.30)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations>=1.0.0->nougat-ocr) (1.4.1)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning>=2.0.0->nougat-ocr) (2.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (1.12)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.5.4->nougat-ocr) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.5.4->nougat-ocr) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.5.4->nougat-ocr) (16.0.6)\n",
            "Collecting h11>=0.8 (from uvicorn<2.0->lightning>=2.0.0->nougat-ocr)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->sconf>=0.2.3->nougat-ocr)\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi<2.0,>=0.92.0->lightning>=2.0.0->nougat-ocr) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi<2.0,>=0.92.0->lightning>=2.0.0->nougat-ocr) (1.1.3)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning>=2.0.0->nougat-ocr) (0.2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning>=2.0.0->nougat-ocr) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning>=2.0.0->nougat-ocr) (67.7.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations>=1.0.0->nougat-ocr) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.5.4->nougat-ocr) (1.3.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, python-editor, xxhash, websockets, ruamel.yaml.clib, readchar, rapidfuzz, python-multipart, pypdfium2, pypdf, orjson, ordered-set, munch, lightning-utilities, h11, dill, blessed, backoff, uvicorn, tiktoken, starlette, ruamel.yaml, multiprocess, Levenshtein, inquirer, huggingface-hub, deepdiff, dateutils, croniter, arrow, transformers, starsessions, sconf, python-Levenshtein, openai, fastapi, lightning-cloud, datasets, torchmetrics, pytorch-lightning, timm, lightning, nougat-ocr\n",
            "Successfully installed Levenshtein-0.21.1 arrow-1.2.3 backoff-2.2.1 blessed-1.20.0 croniter-1.4.1 datasets-2.14.5 dateutils-0.6.12 deepdiff-6.5.0 dill-0.3.7 fastapi-0.103.1 h11-0.14.0 huggingface-hub-0.17.2 inquirer-3.1.3 lightning-2.0.9 lightning-cloud-0.5.38 lightning-utilities-0.9.0 multiprocess-0.70.15 munch-4.0.0 nougat-ocr-0.1.11 openai-0.28.0 ordered-set-4.1.0 orjson-3.9.7 pypdf-3.16.1 pypdfium2-4.20.0 python-Levenshtein-0.21.1 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.9 rapidfuzz-3.3.0 readchar-4.0.5 ruamel.yaml-0.17.32 ruamel.yaml.clib-0.2.7 safetensors-0.3.3 sconf-0.2.5 sentencepiece-0.1.99 starlette-0.27.0 starsessions-1.3.0 tiktoken-0.5.1 timm-0.5.4 tokenizers-0.13.3 torchmetrics-1.1.2 transformers-4.33.2 uvicorn-0.23.2 websockets-11.0.3 xxhash-3.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai tiktoken nougat-ocr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "5exzVTq2y6-g"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = ('PUT_YOUR_API_KEY_HERE')"
      ],
      "metadata": {
        "id": "ay7g639Iy4QN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used CLIP to test. It is a really long paper...\n",
        "[CLIP Paper](https://arxiv.org/abs/2103.00020)"
      ],
      "metadata": {
        "id": "LIOjAad8JB8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "lsFtwxeu-j-G",
        "outputId": "1f5d1a0c-e587-49b6-9f2b-4b6d56b04ce9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-89bef0d2-93a6-4227-85ba-67ce735d130d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-89bef0d2-93a6-4227-85ba-67ce735d130d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2103.00020.pdf to 2103.00020.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# nougat-ocr doing it's magic"
      ],
      "metadata": {
        "id": "-e8a7W92MGga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "uploadfilename = next(iter(uploaded))\n",
        "uploadfilePath = os.path.join(\"/content/\", uploadfilename)"
      ],
      "metadata": {
        "id": "gZc_a3xty5o_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nougat $uploadfilePath -o /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeaFA2nTzBN1",
        "outputId": "f81a84aa-bf68-4980-bce6-57e009640ad8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-09-22 04:36:02.796777: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "  0% 0/12 [00:00<?, ?it/s]WARNING:root:Found repetitions in sample 3\n",
            "INFO:root:Processing file /content/2103.00020.pdf with 48 pages\n",
            "WARNING:root:Skipping page 4 due to repetitions.\n",
            " 75% 9/12 [03:11<01:15, 25.07s/it]WARNING:root:Found repetitions in sample 3\n",
            " 92% 11/12 [03:58<00:24, 24.50s/it]WARNING:root:Found repetitions in sample 1\n",
            "WARNING:root:Found repetitions in sample 3\n",
            "WARNING:root:Skipping page 46 due to repetitions.\n",
            "100% 12/12 [04:24<00:00, 22.05s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = uploadfilename.split('.pdf')[0]+'.mmd'"
      ],
      "metadata": {
        "id": "cZRiVKKPBSk1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support functions"
      ],
      "metadata": {
        "id": "Evn6-QE6MNly"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oLmllz4evQFi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
        "  \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "  try:\n",
        "      encoding = tiktoken.encoding_for_model(model)\n",
        "  except KeyError:\n",
        "      encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  if model == \"gpt-3.5-turbo-0613\":  # note: future models may deviate from this\n",
        "      num_tokens = 0\n",
        "      for message in messages:\n",
        "          num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "          for key, value in message.items():\n",
        "              num_tokens += len(encoding.encode(value))\n",
        "              if key == \"name\":  # if there's a name, the role is omitted\n",
        "                  num_tokens += -1  # role is always required and always 1 token\n",
        "      num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "      return num_tokens\n",
        "  else:\n",
        "      raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
        "  See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JDaPXLH6vQFi"
      },
      "outputs": [],
      "source": [
        "#read text from file and convert to list of messages for openai api\n",
        "\n",
        "def read_text_from_file(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def read_without_references(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        content = file.readlines()\n",
        "\n",
        "    # Define a variable to store the final content without references\n",
        "    final_content = []\n",
        "\n",
        "    # Flag to indicate if the \"References\" section has started\n",
        "    references_started = False\n",
        "\n",
        "    for line in content:\n",
        "        if \"References\" in line:\n",
        "            references_started = True\n",
        "        if not references_started:\n",
        "            final_content.append(line)\n",
        "\n",
        "    return ''.join(final_content)\n",
        "\n",
        "def convert_text_to_messages(text, user_instructions=None):\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert academic journal reviewer in CVPR, IEEE, and ACM.\\\n",
        "        You are reviewing a paper.\\\n",
        "        You need to be constructive to the authors. It is necessary to be critical, but avoid offending the authors.\\\n",
        "        Instead, suggest how they could make the paper better.\\\n",
        "        \"},\n",
        "    # {\"role\": \"user\", \"content\": \"Help me to polish following text.\"},\n",
        "    ]\n",
        "    if user_instructions:\n",
        "        messages.extend([{\"role\": \"user\", \"content\": user_instructions} ])\n",
        "\n",
        "    messages.extend([{\"role\": \"user\", \"content\": text} ])\n",
        "    return messages\n",
        "\n",
        "\n",
        "def convert_messages_to_text(messages):\n",
        "    text = ''\n",
        "    for message in messages:\n",
        "        if message['role'] == 'user':\n",
        "            text += 'Human: ' + message['text'] + '\\n'\n",
        "        elif message['role'] == 'assistant':\n",
        "            text += 'AI: ' + message['text'] + '\\n'\n",
        "    return text\n",
        "\n",
        "def write_text_to_file(text, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(text)\n",
        "\n",
        "def write_messages_to_file(messages, filename):\n",
        "    write_text_to_file(convert_messages_to_text(messages), filename)\n",
        "\n",
        "#count tokens in text\n",
        "def num_tokens_from_text(text, model = \"gpt-3.5-turbo-0613\"):\n",
        "    messages = convert_text_to_messages(text)\n",
        "    return num_tokens_from_messages(messages, model)\n",
        "\n",
        "#read the text file, then return a list of texts whereas each contains less than given number of tokens\n",
        "def split_text_into_list(text, max_tokens=4096):\n",
        "    messages = []\n",
        "    message = ''\n",
        "    for line in text.splitlines():\n",
        "        message_token = num_tokens_from_messages(convert_text_to_messages(message))\n",
        "        line_token = num_tokens_from_messages(convert_text_to_messages(line))\n",
        "        if message_token + line_token < max_tokens/2:\n",
        "            message += line + '\\n'\n",
        "        else:\n",
        "            messages.append(message)\n",
        "            message = line + '\\n'\n",
        "    messages.append(message)\n",
        "    return messages\n",
        "\n",
        "#call openai api to polish the text\n",
        "def instruct_text(text,user_instructions, model = \"gpt-3.5-turbo\"):\n",
        "    messages = convert_text_to_messages(text, user_instructions=user_instructions)\n",
        "    response = openai.ChatCompletion.create(model=model, messages=messages)\n",
        "    return response\n",
        "\n",
        "#Polish an entire txt file then write the results into a new file\n",
        "def summarize_text_file(filename, model = \"gpt-3.5-turbo\"):\n",
        "    text = read_without_references(filename)\n",
        "    paragraphs = split_text_into_list(text, max_tokens=4096)\n",
        "    polished_text = ''\n",
        "    user_instructions = \"The paper is too long. Now you are summarizing it part to part. Provide a summary of given section. Make it shorter but keep all the technical details.\"\n",
        "    for paragraph in paragraphs:\n",
        "        result = instruct_text(paragraph, user_instructions, model)\n",
        "        polished_text += result['choices'][0][\"message\"][\"content\"]\n",
        "    return polished_text\n",
        "\n",
        "def write_text_to_file(text, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(text)\n",
        "\n",
        "def summarize_text_file_to_file(filename, outputFileName, model = \"gpt-3.5-turbo\"):\n",
        "    text = summarize_text_file(filename, model)\n",
        "    write_text_to_file(text, outputFileName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn3XwFwOvQFj"
      },
      "source": [
        "## Cost Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1suKqrmvQFj"
      },
      "source": [
        "Estimate your cost before doing anything for free. This is a rough estimate. No API key required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VtJLq_rvvQFj"
      },
      "outputs": [],
      "source": [
        "def cost_estimation(filename):\n",
        "    text = read_without_references(filename)\n",
        "    messages = convert_text_to_messages(text)\n",
        "    model = \"gpt-3.5-turbo-0613\"\n",
        "    paragraphs = split_text_into_list(text, max_tokens=4096)\n",
        "    estimated_cost = num_tokens_from_messages(messages, model) * (0.0015 + 0.002) / 1000\n",
        "    print(f\"{num_tokens_from_messages(messages, model)} prompt tokens counted.\")\n",
        "    print(f\"Number of paragraphs to process: {len(paragraphs)}\")\n",
        "    print(f\"Estimated cost using gpt-3.5-turbo: ${estimated_cost}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YusGVvS_vQFj",
        "outputId": "30d76509-8adc-4be0-fc44-6a02f4cd87b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24957 prompt tokens counted.\n",
            "Number of paragraphs to process: 14\n",
            "Estimated cost using gpt-3.5-turbo: $0.08734950000000001\n"
          ]
        }
      ],
      "source": [
        "cost_estimation(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Summarizer"
      ],
      "metadata": {
        "id": "SGx9KsUL6BRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_text_file_to_file(filename, 'condensed_paper.txt')"
      ],
      "metadata": {
        "id": "Kj0uQaiJ6AXO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the total token count from the summarized version\n",
        "cost_estimation('condensed_paper.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l6glFHP7Cu2",
        "outputId": "6d58c4dc-3aed-4d02-c719-7aee7a30157e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5400 prompt tokens counted.\n",
            "Number of paragraphs to process: 3\n",
            "Estimated cost using gpt-3.5-turbo: $0.018900000000000004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Longer than 4096 tokens so I will do a double condensation"
      ],
      "metadata": {
        "id": "x1GXo-5ZLTbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_text_file_to_file('condensed_paper.txt', 'condensed_condensed_paper.txt')"
      ],
      "metadata": {
        "id": "GDVqHqR3IKQa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_estimation('condensed_condensed_paper.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhQJ17JtIebb",
        "outputId": "5b611e7f-71d3-40d4-d496-9296a35b8524"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1966 prompt tokens counted.\n",
            "Number of paragraphs to process: 1\n",
            "Estimated cost using gpt-3.5-turbo: $0.006881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Polish an entire txt file then write the results into a new file\n",
        "def instruct_text_file(filename, user_instructions, model = \"gpt-3.5-turbo\"):\n",
        "    text = read_text_from_file(filename)\n",
        "    paragraphs = split_text_into_list(text, max_tokens=4096)\n",
        "    polished_text = ''\n",
        "    for paragraph in paragraphs:\n",
        "        result = instruct_text(paragraph, user_instructions, model)\n",
        "        polished_text += result['choices'][0][\"message\"][\"content\"]\n",
        "    return polished_text"
      ],
      "metadata": {
        "id": "7iRbnZQb6KQg"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start summarize with some instructions"
      ],
      "metadata": {
        "id": "FXrdlhkMMBH4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Summary\n",
        "summary_instructions = \"Now you are reviewing the summarized version of the paper.\\\n",
        "Give a concise summary of the paper for the literature review. \\\n",
        "Answer following questions in the summary:\\\n",
        "What problem is addressed in the paper?\\\n",
        "Is it a new problem? If so, why does it matter? If not, why does it still matter?\\\n",
        "What is the key to the solution? What is the main contribution?\\\n",
        "Do the experiments sufficiently support the claims?\"\n",
        "\n",
        "summary = instruct_text_file('condensed_condensed_paper.txt', summary_instructions)\n",
        "\n",
        "#strengths\n",
        "strengths_instructions = \"Now you are reviewing the summarized version of the paper.\\\n",
        "Give a concise statement of the strengths of the paper for the literature review. \\\n",
        "What about the paper provides value - - interesting ideas that are experimentally validated, an insightful organization of related work, new tools,\\\n",
        "impressive results. Most importantly, what can someone interested in the topic learn from the paper. What are the key contributions and why do they matter?\\\n",
        "Use bullet points to write the strengths statement.\"\n",
        "\n",
        "strengths = instruct_text_file('condensed_condensed_paper.txt', strengths_instructions)\n",
        "\n",
        "#weaknesses\n",
        "weaknesses_instructions = \"Now you are reviewing the summarized version of the paper.\\\n",
        "Give a concise statement of the weaknesses of the paper for the literature review. \\\n",
        "What detracts from the contributions -- Does the paper lack controlled experiments to validate the contributions?\\\n",
        "Are there misleading claims or technical errors? Is it possible to understand (and ideally reproduce) the method and experimental \\\n",
        "setups by reading the paper?What are the key contributions and why do they matter? What aspects of the paper most need improvement?\\\n",
        "Use bullet points to write the weaknesses statement.\"\n",
        "\n",
        "weaknesses = instruct_text_file('condensed_condensed_paper.txt', weaknesses_instructions)\n",
        "\n",
        "final_summary = '**Summary**'+'\\n'+summary + '\\n\\n' + '**Strengths**'+'\\n'+strengths + '\\n\\n' + '**Weaknesses**'+'\\n'+weaknesses\n",
        "\n",
        "with open('literature_review.txt', 'w') as file:\n",
        "    file.write(final_summary)\n",
        "\n"
      ],
      "metadata": {
        "id": "kNcY8Bqa7aLH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "6M8vdNAjL99i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUeSDXif-VqA",
        "outputId": "3772b449-9248-4d27-db4e-cda31f8bdf4a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Summary**\n",
            "In this section, the authors evaluate the zero-shot transfer capabilities of CLIP on various datasets. They compare its performance to a baseline logistic regression model trained on ResNet-50 features. The results show that CLIP performs well on general object classification tasks but struggles on specialized tasks. The authors suggest using CLIP as a prior for few-shot learning to improve its performance. They analyze the estimated data efficiency of zero-shot transfer and find that it varies widely across datasets. The experiments provide support for the claims made by the authors.\n",
            "\n",
            "In the evaluation of CLIP on the FairFace dataset, the authors compare the performance of zero-shot CLIP and logistic regression CLIP to other models. LR CLIP achieves higher accuracy on the FairFace dataset. The authors also test the model's performance across intersectional race and gender categories and find that it achieves high accuracy on gender classification for all race categories. However, certain demographic subgroups are more likely to be misclassified into non-human or crime-related classes.\n",
            "\n",
            "The authors investigate the impact of class design on model performance and biases. They show that adding the \"child\" category reduces misclassifications of people under 20 into crime-related and non-human animal categories. The authors also conduct experiments on gender classification using images of Members of Congress, where the model achieves improved accuracy compared to the FairFace dataset. Additionally, the authors explore how varying thresholds for label probabilities impact the presence of biases.\n",
            "\n",
            "The evaluation of CLIP on surveillance tasks shows that it performs well for coarse classification but poorly for fine-grained detection and identity detection. The authors suggest that CLIP could be suitable for bespoke surveillance use cases.\n",
            "\n",
            "The related work section provides an overview of existing literature in natural language supervision and its applications in computer vision tasks. However, it would benefit from more concise summaries of each work and a clearer discussion of the authors' own findings.\n",
            "\n",
            "Overall, the experiments sufficiently support the claims made by the authors. However, the paper could be improved by providing more detailed explanations of the experimental setup, evaluation metrics, and the significance of the performance differences observed. Additionally, discussing the reasons behind the underperformance of CLIP on certain specialized tasks and exploring potential future improvements would enhance the paper.\n",
            "\n",
            "**Strengths**\n",
            "- The paper provides a comprehensive evaluation of CLIP's zero-shot transfer capabilities on various datasets, highlighting its strengths and weaknesses across different task domains.\n",
            "- CLIP performs well on general object classification tasks like ImageNet and CIFAR10/100, achieving state-of-the-art performance on STL10.\n",
            "- It outperforms ResNet-50 on action recognition datasets such as Kinetics700 and UCF101.\n",
            "- The paper analyzes the estimated data efficiency of zero-shot transfer across datasets, providing valuable insights on the number of labeled examples required per class.\n",
            "- The paper also investigates the impact of class design on the performance and biases of the CLIP model, highlighting the importance of this aspect in determining model behavior.\n",
            "- The authors explore biases in classification performance across intersectional race and gender categories and for harmful classification terms, shedding light on important ethical considerations.\n",
            "- The paper addresses the potential implications of general-purpose computer vision models in the surveillance domain, highlighting societal concerns and the need for responsible AI development.\n",
            "- The authors emphasize the importance of design decisions in shaping biases and determining the manifestation of biases in model outputs.\n",
            "- The paper encourages further research to characterize the capabilities, shortcomings, and biases of general-purpose computer vision models, providing suggestions for future directions and evaluation tests.\n",
            "\n",
            "Overall, the paper's strengths lie in its comprehensive evaluation, analysis of biases, and the exploration of ethical considerations and potential societal implications. The findings contribute to the understanding of CLIP's transfer capabilities and the challenges involved in developing responsible AI systems for computer vision.\n",
            "\n",
            "**Weaknesses**\n",
            "- The paper lacks controlled experiments to validate the contributions of CLIP's zero-shot transfer capabilities, making it difficult to assess the reliability of the reported results.\n",
            "- The paper would benefit from more detailed explanations of the experimental setup, evaluation metrics, and the significance of the performance differences observed across different datasets and tasks.\n",
            "- The authors should provide more detailed explanations and examples of the observed limitations of CLIP's generalization and flexibility, as well as the potential biases and disparities in its classification performance across intersectional race and gender categories.\n",
            "- The paper could be improved by discussing potential reasons behind the underperformance of CLIP on certain specialized tasks and providing insights for future improvements in these areas.\n",
            "- The conclusion section should include a brief summary of the investigation of transfer from NLP pre-training to computer vision and the specific findings from this investigation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmgDa9-QI650"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}